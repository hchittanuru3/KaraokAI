{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from sys import argv\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "\t\tdef __init__(self):\n",
    "\t\t\tsuper(CNNModel, self).__init__()\n",
    "\t\t\tself.cnn1 = nn.Conv1d(in_channels=1025, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
    "\t\t\t#self.nl1 = nn.ReLU()\n",
    "\t\t\t#self.pool1 = nn.AvgPool1d(kernel_size=5)\n",
    "\t\t\t#self.fc1 = nn.Linear(4096*2500,2**5)\n",
    "\t\t\t#self.nl3 = nn.ReLU()\n",
    "\t\t\t#self.fc2 = nn.Linear(2**10,2**5)\n",
    "\t\t\n",
    "\t\tdef forward(self, x):\n",
    "\t\t\tout = self.cnn1(x)\n",
    "\t\t\t#out = self.nl1(out)\n",
    "\t\t\t#out = self.pool1(out)\n",
    "\t\t\tout = out.view(out.size(0),-1)\n",
    "\t\t\t#out = self.fc1(out)\n",
    "\t\t\t#out = self.nl3(out)\n",
    "\t\t\t#out = self.fc2(out)\n",
    "\t\t\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\ta, b, c = input.size()  # a=batch size(=1)\n",
    "        # b=number of feature maps\n",
    "        # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\t\tfeatures = input.view(a * b, c)  # resise F_XL into \\hat F_XL\n",
    "\t\tG = torch.mm(features, features.t())  # compute the gram product\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "\t\treturn G.div(a * b * c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "\n",
    "\tdef __init__(self, target, weight):\n",
    "\t\tsuper(StyleLoss, self).__init__()\n",
    "\t\tself.target = target.detach() * weight\n",
    "\t\tself.weight = weight\n",
    "\t\tself.gram = GramMatrix()\n",
    "\t\tself.criterion = nn.MSELoss()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.output = input.clone()\n",
    "\t\tself.G = self.gram(input)\n",
    "\t\tself.G.mul_(self.weight)\n",
    "\t\tself.loss = self.criterion(self.G, self.target)\n",
    "\t\treturn self.output\n",
    "\n",
    "\tdef backward(self,retain_graph=True):\n",
    "\t\tself.loss.backward(retain_graph=retain_graph)\n",
    "\t\treturn self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tN_FFT=2048\n",
    "\tdef read_audio_spectum(filename):\n",
    "\t\tx, fs = librosa.load(filename, duration=58.04) # Duration=58.05 so as to make sizes convenient\n",
    "\t\tS = librosa.stft(x, N_FFT)\n",
    "\t\tp = np.angle(S)\n",
    "\t\tS = np.log1p(np.abs(S))  \n",
    "\t\treturn S, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tstyle_layers_default = ['conv_1']\n",
    "\n",
    "\tstyle_weight=2500\n",
    "def get_style_model_and_losses(cnn, style_float,style_weight=style_weight, style_layers=style_layers_default): #STYLE WEIGHT\n",
    "\t\t\n",
    "\t\tcnn = copy.deepcopy(cnn)\n",
    "\t\tstyle_losses = []\n",
    "\t\tmodel = nn.Sequential()  # the new Sequential module network\n",
    "\t\tgram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "\t\tif torch.cuda.is_available():\n",
    "\t\t\tmodel = model.cuda()\n",
    "\t\t\tgram = gram.cuda()\n",
    "\n",
    "\t\tname = 'conv_1'\n",
    "\t\tmodel.add_module(name, cnn.cnn1)\n",
    "\t\tif name in style_layers:\n",
    "\t\t\ttarget_feature = model(style_float).clone()\n",
    "\t\t\ttarget_feature_gram = gram(target_feature)\n",
    "\t\t\tstyle_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "\t\t\tmodel.add_module(\"style_loss_1\", style_loss)\n",
    "\t\t\tstyle_losses.append(style_loss)\n",
    "\n",
    "\t\t#name = 'pool_1'\n",
    "\t\t#model.add_module(name, cnn.pool1)\n",
    "\n",
    "\t\t'''name = 'fc_1'\n",
    "\t\tmodel.add_module(name, cnn.fc1)\n",
    "\n",
    "\t\tname = 'nl_9'\n",
    "\t\tmodel.add_module(name, cnn.nl9)\n",
    "\n",
    "\t\tname = 'fc_2'\n",
    "\t\tmodel.add_module(name, cnn.fc2)'''\n",
    "\n",
    "\t\treturn model, style_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tlearning_rate_initial = 0.03\n",
    "\n",
    "\tdef get_input_param_optimizer(input_float):\n",
    "\t\tinput_param = nn.Parameter(input_float.data)\n",
    "\t\t#optimizer = optim.Adagrad([input_param], lr=learning_rate_initial, lr_decay=0.0001,weight_decay=0)\n",
    "\t\toptimizer = optim.Adam([input_param], lr=learning_rate_initial, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\t\treturn input_param, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps= 2500\n",
    "\n",
    "def run_style_transfer(cnn, style_float, input_float, num_steps=num_steps, style_weight=style_weight): #STYLE WEIGHT, NUM_STEPS\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses= get_style_model_and_losses(cnn, style_float, style_weight)\n",
    "    input_param, optimizer = get_input_param_optimizer(input_float)\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "\n",
    "    while run[0] <= num_steps:\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_param.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                #print('sl is ',sl,' style loss is ',style_score)\n",
    "                style_score += sl.backward()\n",
    "#             print(style_score.item())\n",
    "            run[0] += 1\n",
    "            if run[0] % 100 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print(style_score)\n",
    "                print('Style Loss : {:8f}'.format(style_score.item())) #CHANGE 4->8 \n",
    "                print()\n",
    "\n",
    "            return style_score\n",
    "\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    input_param.data.clamp_(0, 1)\n",
    "    return input_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Rates are same\n",
      "Building the style transfer model..\n",
      "Optimizing..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-453b682f08c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d38e4a4d4cc8>\u001b[0m in \u001b[0;36mrun_style_transfer\u001b[0;34m(cnn, style_float, input_float, num_steps, style_weight)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0minput_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs7643/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d38e4a4d4cc8>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstyle_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m#print('sl is ',sl,' style loss is ',style_score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mstyle_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#             print(style_score.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mrun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-78bb4a14d54d>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, retain_graph)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs7643/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/cs7643/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print('Enter the names of SCRIPT, Content audio, Style audio')\n",
    "content_audio_name = \"./weeknd_10sec_sample.wav\"\n",
    "style_audio_name = \"./beyonce_10sec_sample.wav\"\n",
    "\n",
    "# USING LIBROSA\n",
    "\n",
    "\n",
    "style_audio, style_sr = read_audio_spectum(style_audio_name)\n",
    "content_audio, content_sr = read_audio_spectum(content_audio_name)\n",
    "\n",
    "if(content_sr == style_sr):\n",
    "    print('Sampling Rates are same')\n",
    "else:\n",
    "    print('Sampling rates are not same')\n",
    "    exit()\n",
    "\n",
    "num_samples=style_audio.shape[1]\t\n",
    "\n",
    "style_audio = style_audio.reshape([1,1025,num_samples])\n",
    "content_audio = content_audio.reshape([1,1025,num_samples])\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    style_float = Variable((torch.from_numpy(style_audio)).cuda())\n",
    "    content_float = Variable((torch.from_numpy(content_audio)).cuda())\t\n",
    "else:\n",
    "    style_float = Variable(torch.from_numpy(style_audio))\n",
    "    content_float = Variable(torch.from_numpy(content_audio))\n",
    "#style_float = style_float.unsqueeze(0)\n",
    "\n",
    "#style_float = style_float.view([1025,1,2500])\n",
    "\n",
    "'''\n",
    "print(style_float.size())\n",
    "exit()\n",
    "'''\n",
    "#style_float = style_float.unsqueeze(0)\n",
    "#content_float = content_float.unsqueeze(0)\n",
    "#content_float = content_float.reshape(1025,1,2500)\n",
    "\n",
    "#content_float = content_float.unsqueeze(0)\n",
    "#content_float = content_float.squeeze(0)\n",
    "\n",
    "\n",
    "\n",
    "cnn = CNNModel()\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "\n",
    "\n",
    "\n",
    "input_float = content_float.clone()\n",
    "#input_float = Variable(torch.randn(content_float.size())).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "output = run_style_transfer(cnn, style_float, input_float)\n",
    "if torch.cuda.is_available():\n",
    "    output = output.cpu()\n",
    "\n",
    "#output = output.squeeze(0)\n",
    "output = output.squeeze(0)\n",
    "output = output.numpy()\n",
    "#print(output.shape)\n",
    "#output = output.resize([1025,2500])\n",
    "\n",
    "N_FFT=2048\n",
    "a = np.zeros_like(output)\n",
    "a = np.exp(output) - 1\n",
    "\n",
    "# This code is supposed to do phase reconstruction\n",
    "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
    "for i in range(500):\n",
    "    S = a * np.exp(1j*p)\n",
    "    x = librosa.istft(S)\n",
    "    p = np.angle(librosa.stft(x, N_FFT))\n",
    "\n",
    "OUTPUT_FILENAME = 'output1D_4096_iter'+str(num_steps)+'_c'+content_audio_name+'_s'+style_audio_name+'_sw'+str(style_weight)+'_k3s1p1.wav'\n",
    "librosa.output.write_wav(OUTPUT_FILENAME, x, style_sr)\n",
    "\n",
    "print('DONE...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs7643)",
   "language": "python",
   "name": "cs7643"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
